---
title: "Práctica 3 - Parte I"
author: "A. Verri Kozlowski y F. Patitucci"
subtitle: Aprendizaje Estadístico -  (84:04)
output:
  html_document:
    toc: yes
    df_print: paged
  pdf_document:
    toc: yes
    number_sections: no
    latex_engine: xelatex
    keep_tex: no
---


```{r setup, include=FALSE,echo=FALSE}
# lib ----
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(data.table) # data,frame extendido
library(ggthemes) 
library(flextable) # Plot Tables
# knit options ----
knitr::opts_chunk$set(out.width="50%",fig.align = "center",eval=TRUE, echo = FALSE, warning = FALSE, size = "small")
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

# out_type <- knitr::opts_knit$get("rmarkdown.pandoc.to")
getOutputFormat <- function() {
  output <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
    )$output
  if (is.list(output)){
    return(names(output)[1])
  } else {
    return(output[1])
  }
}




# rnd  
set.seed(25111970)


# shiny ----
plotNoise = function() {
  library(shiny)  
  shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(width=2, 
               numericInput(inputId = 'n', label = 'n=',min=100,max=100000,value=100 )
               ),
        column(width=5, 
               sliderInput(inputId = 'log10SD', label='log10(sd)=',min=-4,max=0,value = log10(0.025),step=0.1)
               )
      ),
      fluidRow(
        plotOutput('noise', height = "200px")  
      )
    ),
    server = function(input, output, session) {
      output$noise = renderPlot(height = 400, {
        req(input$log10SD,input$n)
        S <- 10^(as.double(input$log10SD))
        N <- as.integer(input$n)
        e <- rnorm(n=N,mean=0,sd=S)
        X <- rnorm(n=N,mean=0,sd=1)
        Y <- -1 + 0.5*X + e
        DT <- data.table(x=X,y=Y)
        MDL <- lm(data=DT,y~1+x)
        b0 <- MDL$coefficients["(Intercept)"]
        b1 <- MDL$coefficients["x"]
        DTP <- data.table(x=X,y=b0+b1*X)
        OUT <- ggplot() + 
          geom_point(data=DT, aes(x=x, y=y),color="plum") + 
          geom_line(data=DTP,aes(x=x, y=y),color="salmon")+ 
          geom_abline(intercept = -1,slope = 0.5,color="blue") + 
          theme_bw()
        return(OUT)
      })
    },
    options = list(height = 400)
  )
}
# equatags ----
if(!equatags::mathjax_available()){
  equatags::mathjax_install()
}
# flextable  ----
use_df_printer()
set_flextable_defaults(
  font.size = 10,border.color = "gray",
  padding.bottom = 3,   padding.top = 3,
  padding.left = 4,  padding.right = 4,
  post_process_html = function(x){
    # theme_booktabs(x)  |>
    theme_vanilla(x) |>
      set_table_properties(layout = "autofit") |>
      autofit() |>
      align(align="center",part = "header") |>
      bold(part = "header")  |> 
      fontsize(size = 10, part = "header")|> 
      fontsize(size = 10, part = "body")
  },
  post_process_pdf = function(x){
    # theme_booktabs(x)  |>
      theme_vanilla(x) |>
      set_table_properties(layout = "autofit") |>
      autofit() |>
      align(align="center",part = "header") |>
      bold(part = "header")  |>
      fontsize(size = 9, part = "header")|> 
      fontsize(size = 9, part = "body")
  },
  post_process_docx = function(x){
    theme_vanilla(x) |>
      set_table_properties(layout = "autofit") |>
      autofit() |>
      align(align="center",part = "header") |>
      bold(part = "header")  |>
      fontsize(size = 10, part = "header")|> 
      fontsize(size = 9, part = "body")
  }
) 
```

## Ejercicio 1 {-}

Implementar una función que dado un vector `y` de valores de respuesta y una matriz `x` de valores observados, mediante las ecuaciones normales, calcule el estimador de cuadrados mínimos


_Se plantea el siguiente algoritmo para el cálculo del estimador de mínimos cuadrados,que toma como argumentos un vector 'y' y una matriz o vector columna 'x', y devuelve una tabla de coeficientes 'b0,b1,...'_

```{r echo=TRUE}
Coeff <- function(data=NULL,y,x=NULL,intercept=TRUE){
  if(!is.null(data) & is.character(y)){ 
    Y <- data[[y]] 
    XCOLS <- colnames(data)[colnames(data)!=y]
    x <- data[,..XCOLS] |> as.matrix()
  } 
  if(is.null(data) & !is.null(x) & is.vector(y)){ 
    Y <- y
    if(!is.matrix(x)){x <- as.matrix(x)}
  } 
  if(intercept) {X <- cbind(1,x)} else {X <- x}
  B <- solve(t(X)%*%X)%*%t(X)%*%Y |> t() |> as.data.table()
  p <- ncol(x)
  if(intercept){
    #b0,b1,..bp
    ID <- paste0("b",seq(0,p))} 
  else {
    ID <- paste0("b",seq(1,p))}
  setnames(B,new=ID)
  return(B)
}
```


\newpage
## Ejercicio 2 {-}

Se tiene en el archivo `girasol.txt` el rinde de diversas parcelas de girasol (en toneladas) según la
cantidad de dinero invertida en fertilizantes (en miles de pesos).



a Graficar en un diagrama de dispersión inversión vs rinde.
b. Plantear un modelo de regresión lineal simple obtener el estimador de mı́nimos cuadrados.
c. Graficar la recta de regresión obtenida, ¿detecta algo sospechoso?

_El modelo de regresion lineal se obtiene a partir de la función 'Coeff()' definida en el ejercicio 1, La figura siguiente muestra el gráfico de dispersión de los puntos y la recta de ajuste_

```{r}
DT <-fread(file=file.path("data","girasol.txt"))
COEFF <- Coeff(data=DT,y="rinde")
ggplot(data=DT) +
  geom_point(mapping =aes(x=inversion, y=rinde),color="blue") +
  labs(title="Diagrama de dispersion",x="Inversion",y = "Rinde") +
  geom_abline(mapping = aes(intercept = COEFF$b0, slope = COEFF$b1),color = "red")+
  theme_bw()
```

_Se puede observar que hay `r nrow(DT[rinde==5.0])` observaciones con un valor fijo `(rinde=5)` que está muy lejos del promedio en el resto de las observaciones, y atraen a la recta de ajuste. Si se asumen como "outliers" y se excluyen estas observaciones, los estimadores ajustan mejor por el promedio_

```{r}
COEFF <- Coeff(data=DT[rinde!=5.0],y="rinde")
# 
# ggplot(data=DT,aes(x=rinde )) + geom_boxplot(notch=FALSE,color="blue")+
#   theme_bw()

ggplot(data=DT) +
  geom_point(mapping =aes(x=inversion, y=rinde),color="blue") +
  labs(title="Diagrama de dispersión",x="Inversión",y = "Rinde") +
  geom_abline(mapping = aes(intercept =COEFF$b0, slope = COEFF$b1),color = "red")+
  theme_bw()
  
```

\newpage
## Ejercicio 3 {-}

Considerar el archivo `abalone.txt` que contiene información sobre distintas muestras de abalones. Los atributos están separados por coma, con los siguientes campos: 

- Sexo (categórica): M (masculino), F (femenino) o I (infante).
- Longitud (continua), en milimetros.
- Diametro (continua), en milimetros.
- Altura (continua), en milimetros.
- Peso completo del abalone (continua), en gramos.
- Peso de la carne (continua), en gramos.
- Peso de las vísceras (continua), en gramos.
- Peso del caparazón (continua), en gramos.
- Anillos (discreta).

  a. Plantear un modelo de regresión lineal simple para predecir el diámetro `D` en función de la longitud `L`.


_Se plantea un modelo de regresión simple con la función 'Coeff()', donde en este caso $x$ es la longitud del abalone y $y$ es el diámetro del mismo._

```{r echo=FALSE}
DT <- fread(
  file=file.path("data","abalone.txt"),
  col.names = c("S","L","D","H","P","P_carne","P_visceras","P_caparazon","anillos"))

COEFF <- Coeff(y=DT$D,x=DT$L)
ggplot(data=DT) +   
  geom_point(aes(x=L, y=D),color= "lightblue") +  
  labs(title="",x="Longitud",y = "Diametro") +
  geom_abline(mapping = aes(intercept = COEFF$b0, slope = COEFF$b1),color = "orangered4")+
  theme_bw()

```

  b. Observar que el conjunto de datos tiene información del peso total de cada espécimen junto con un desagregado por partes. Ajustar un modelo de regresión múltiple que explique el peso total `P` en función del peso del caparazón `P_caparazon`, las vı́sceras `P_visceras` y la carne `P_carne` .

```{r echo=TRUE}
COEFF <- Coeff(data=DT[,c("P","P_caparazon","P_visceras","P_carne")],y="P")
```
_En este caso, igual que en el inciso anterior se utilizó la función 'Coeff()', seleccionando del dataset unicamente `P_caparazon`s `P_visceras` y `P_carne` como variables dependientes. Los estimadores del modelo resultaron  $\hat\beta_0=$ `r COEFF$b0`(primer valor), $\hat\beta_1=$ `r COEFF$b1`(pendiente asociada al peso del caparazón), $\hat\beta_2=$ `r COEFF$b2`(pendiente asociada al peso de las vísceras) y $\hat\beta_3=$ `r COEFF$b3` (pendiente asociada al peso de la carne)._


  c. Se trata ahora de establecer una relación entre el peso total `P` y el diámetro del espécimen `D`. Empezar dibujando en un scatter plot ambas variables. Si definimos como `P` al peso total y `D` al diámetro, se consideran los siguientes modelos:

- Modelo lineal simple, $P = b + a\.D + \varepsilon$
- Modelo cuadrático, $P = c + b\.D + a \. D^2 + \varepsilon$
- Modelo cúbico sin términos de orden inferior, $P = a \. D^3 + \varepsilon$

  Efectuar en cada caso una regresión y graficar las curvas superpuestas sobre el scatter plot.

De igual manera que en los incisos anteriores se utilizó la función `Coeff()` para obtener los estimadores de cada uno de los casos. Para el modelo lineal simple, $\hat{P} = \hat{\beta_0} + \hat{\beta_1} D$, donde $P$ es el peso completo del abalone y $D$ es el diámetro

```{r echo=TRUE}
DATA <- DT[,.(P=P,x1=D)]
M1 <- Coeff(data=DATA,y="P")
P1 <- DT[,.(x=D,y=M1$b0+M1$b1*D)] #Predictor
```

_Para el modelo cuadrático,  el estimador adoptado fue $\hat{P} = \hat{\beta_0} + \hat{\beta_1} D+\hat{\beta_2} D^2$_ 
```{r echo=TRUE}
DATA <- DT[,.(P=P,x1=D,x2=D^2)]
M2 <- Coeff(data=DATA,y="P")
P2 <- DT[,.(x=D,y=M2$b0+M2$b1*D+M2$b2*D^2)] #Predictor
```

_Para el modelo cúbico incompleto el estimador adoptado fue $\hat{P} = \hat{\beta_0} + \hat{\beta_1} D^3$. En este caso no podemos usar el algoritmo `Coeff()` con sus parámetros por defecto, sino que debemos especificar que el término independiente es nulo mediante el parámetro `intercept=FALSE`_ 

```{r echo=TRUE}
DATA <- DT[,.(P=P,x1=D^3)]
M3 <- Coeff(data=DATA,y="P",intercept=FALSE)
P3  <- DT[,.(x=D,y=M3$b1*D^3)] #Predictor
```
```{r}
ggplot() +   
  geom_point(data=DT[,.(P,D)],aes(x=D, y=P),color= "plum") +  
  labs(title="",y="Peso",x = "Diametro") +
  geom_line(data=P1,aes(x=x,y=y,color = "Lineal (M1)"))+
  geom_line(data=P2,aes(x=x,y=y,color = "Cuadrático (M2)"))+
  geom_line(data=P3,aes(x=x,y=y,color = "Cúbico (M3)"))+
  scale_colour_manual(values = c("Lineal (M1)" = "orangered1","Cuadrático (M2)" = "orangered3","Cúbico (M3)" = "orangered4"))+
  theme_bw()
```

\newpage
## Ejercicio 4 {-}
En este ejercicio se crearán datos simulados y se ajustará un modelo de regresión lineal simple.
 
  a) Utilizando la función rnorm, crear un vector x que contenga 100 observaciones provenientes de una distribución $X \sim \mathcal{N}(\mu=0,\,\sigma=1)$
  b) Utilizando la función rnorm, crear un vector epsilon que contenga 100 observaciones provenientes de una distribución $\Sigma \sim \mathcal{N}(\mu=0,\sigma=0.025)$
  c) Usando $x$ y $\varepsilon$, generar un vector acorde al modelo: $y = −1 + 0.5 x + \varepsilon$. ¿Cuál es la longitud del vector y? ¿Cuáles son los valores de $\beta_0$ y $\beta_1$ en el modelo?
  
```{r echo=TRUE}
X <- rnorm(n=100,mean=0,sd=1)
e <- rnorm(n=100,mean=0,sd=0.025)
Y <- -1 + 0.5*X +e
```
_La longitud del vector `Y=-1+0.5*X+e` es igual a 100  ya que fue construido como la suma de dos vectores de 100 elementos, y además R tiene sobrecargado el operador suma y permite agregar la constante `1` a cada elemento del vector. Los parámetros $\beta_0$ y $\beta_1$ del modelo son los coeficientes de regresión del modelo, y para este caso resultan $\beta_0=-1$ y $\beta_1=+0.5$_  
  
  d) Realizar un scatterplot y observar la relación entre x e y.
  
```{r }
DT <- data.table(x=X,y=Y)
ggplot(data=DT, aes(x=x, y=y)) + geom_point(color="tomato") + theme_bw()
```
  
e) Ajustar un modelo lineal para predecir y en función de x utilizando el método de cuadrados mı́nimos.

```{r echo=TRUE}
# DT <- data.table(x=X,y=Y)
# MDL <- lm(data=DT,y~x)
COEFF <- Coeff(x=X,y=Y)
# b0 <- MDL$coefficients["(Intercept)"]
# b1 <- MDL$coefficients["x"]
b0 <- COEFF$b0
b1 <- COEFF$b1


```

Comparar los valores exactos de $\beta_0$ Y $\beta_1$ con sus estimaciones. 
```{r}
data.table("\\beta_0 / \\hat{\\beta_0}"=-1/b0,"\\beta_1 / \\hat{\\beta_1}"=0.5/b1) |> 
  flextable() |>  colformat_double(digits = 4) |> 
  mk_par(part = "header", value = as_paragraph(as_equation(.)),use_dot = TRUE) |>    autofit()
```
 
_Los estimadores de $\beta_0$ y $\beta_1$ resultaron $\hat{\beta_0}\approx$ `r b0` y $\hat{\beta_1}\approx$ `r b1`, respectivamente. La varianza del modelo $\sigma^2$ en este caso es conocida y es igual a $\sigma^2=var[\varepsilon]=$ `var(e)=` `r var(e)`._ 


_El estimador de la varianza $s^2$ se define como:_

$$s^2 = \frac{RSS} {n-2}=\frac{\sum{(y_i-\hat{\beta_0}-\hat{\beta_1} x_i)^2}} {n-2}$$
```{r echo=TRUE}
y <- b0+b1*X
Y <- -1+0.5*X+e
n <- length(Y) #100
RSS <- t(Y-y)%*%(Y-y)
s2 <- (RSS/(n-2)) 
```

_y resulta igual a $s^2\approx$ `r s2`._

f) Graficar la recta de cuadrados mı́nimos sobre el gráfico realizado en (d). En otro color graficar la recta $Y = −1 + 0,5X$

```{r }
DT <- data.table(x=X,y=Y)
DTP <- data.table(x=X,y=b0+b1*X) #Predictor
B0 <- -1
B1 <- 0.5

ggplot() + 
  geom_point(data=DT, aes(x=x, y=y),color="tomato") + 
  geom_line(data=DTP,aes(x=x, y=y),color="red")+ 
  geom_abline(intercept = B0,slope = B1,color="blue") + theme_bw()
```
_Las lineas son prácticamente idénticas y no hay manera de diferenciarlas, tal como se vio en la comparación de los coeficientes_


g) Ajustar un modelo polinomial que prediga $y$ usando $x$ y $x^2$ . 

```{r echo=TRUE}
DT <- data.table(y=Y,x1=X,x2=X^2)
MDL <- lm(data=DT,y~x1+x2)
b0 <- MDL$coefficients["(Intercept)"]
b1 <- MDL$coefficients["x1"]
b2 <- MDL$coefficients["x2"]
```
_Los estimadores de $\beta_0$ y $\beta_1$ ahora resultan en  $\hat{\beta_0}\approx$ `r b0` y $\hat{\beta_1}\approx$ `r b1` respectivamente y se agrega un tercer estimador para el término cuadrático igual a $\hat{\beta_2}\approx$ `r b2`_

```{r}

data.table("\\beta_0/\\hat{\\beta_0}"=B0/b0,"\\beta_1/\\hat{\\beta_1}"=B1/b1) |> 
  flextable() |>  colformat_double(digits = 4) |> 
  mk_par(part = "header", value = as_paragraph(as_equation(.)),use_dot = TRUE) |>    autofit()
```


  ¿Encuentra alguna evidencia de que el término cuadrático mejora el ajuste del modelo? 

_No, los estimadores $\hat{\beta_0}\approx$ `r b0` y $\hat{\beta_1}\approx$ `r b1` prácticamente no difieren de los anteriores._

h) Repetir los ı́tems (a) a (f) modificando los datos generados de manera que haya menos ruido en los datos. Una forma de hacerlo es disminuyendo el valor de la varianza de la distribución normal usada para general el término del error epsilon. 

```{r}
e <- rnorm(n=100,mean=0,sd=0.025/5)
Y <- -1 + 0.5*X + e
DT <- data.table(x=X,y=Y)
MDL <- lm(data=DT,y~x)
b0 <- MDL$coefficients["(Intercept)"]
b1 <- MDL$coefficients["x"]

y <- b0 + b1*X 
DTP <- data.table(x=X,y=y)
RSS <- t(Y-y)%*%(Y-y)
s2 <- (RSS/(n-2))

```

```{r}
if(knitr::is_latex_output()){
  ggplot() + geom_point(data=DT, aes(x=x, y=y),color="plum") + geom_line(data=DTP,aes(x=x, y=y),color="salmon")+ geom_abline(intercept = B0,slope = B1,color="blue") + theme_bw()
}
```

_Reduciendo la varianza cinco veces $(\varepsilon\sim \mathcal{N}(0,0.005)$,los estimadores de $\beta_0$ y $\beta_1$ ahora resultan en $\hat{\beta_0}\approx$ `r b0` y $\hat{\beta_1}\approx$ `r b1` respectivamente En este caso, la varianza $\sigma$ resulta igual a $\sigma^2 \approx$ `r var(e)` y su estimador resulta igual a $s^2 \approx$ `r s2`_ 


```{r}
data.table("\\beta_0 / \\hat{\\beta_0}"=B0/b0,"\\beta_1 / \\hat{\\beta_1}"=B1/b1) |> 
  flextable() |>  colformat_double(digits = 4) |> 
  mk_par(part = "header", value = as_paragraph(as_equation(.)),use_dot = TRUE) |>    autofit()
```


i) Repetir los ı́tems (a) a (f) modificando los datos generados de manera que haya más ruido en los datos. Una forma de hacerlo es aumentando el valor de la varianza de la distribución normal usada para general el término del error epsilon. 


```{r }
e <- rnorm(n=100,mean=0,sd=0.025*10)
Y <- -1 + 0.5*X + e
DT <- data.table(x=X,y=Y)
MDL <- lm(data=DT,y~x)
b0 <- MDL$coefficients["(Intercept)"]
b1 <- MDL$coefficients["x"]
DTP <- data.table(x=X,y=b0+b1*X)

y <- b0 + b1*X 
DTP <- data.table(x=X,y=y)
RSS <- t(Y-y)%*%(Y-y)
s2 <- (RSS/(n-2))
```

```{r}
if(knitr::is_latex_output()){
  ggplot() + geom_point(data=DT, aes(x=x, y=y),color="plum") + geom_line(data=DTP,aes(x=x, y=y),color="salmon")+ geom_abline(intercept = B0,slope = B1,color="blue") + theme_bw()
}
```

_Aumentando la varianza de $X$ diez veces$(\varepsilon\sim \mathcal{N}(0,0.25)$,los estimadores de $\beta_0$ y $\beta_1$ ahora resultan en  $\hat{\beta_0}\approx$ `r b0` y $\hat{\beta_1}\approx$ `r b1` respectivamente. En este caso, la varianza resulta igual a $\sigma^2 \approx$ `r var(e)` y su estimador resulta igual a $s^2 \approx$ `r s2`_ 

```{r}
data.table("\\beta_0 / \\hat{\\beta_0}"=B0/b0,"\\beta_1 / \\hat{\\beta_1}"=B1/b1) |> 
  flextable() |>  colformat_double(digits = 4) |> 
  mk_par(part = "header", value = as_paragraph(as_equation(.)),use_dot = TRUE) |>    autofit()
```

```{r}
if(knitr::is_html_output()){
   plotNoise()
} 
```

j) En ambos escenarios, hallar una estimación de la varianza. 



\newpage
## Ejercicio 5 {-}

  a) Generar el siguiente modelo: Crear dos vectores de datos  $x_1$ y $x_2$ de tamaño 100 con una distribución $X \sim\mathcal{U}(0,1)$  y crear un vector $y = 2 + 2 ∗ x_1 + 0,3 ∗ x_2 + \varepsilon$, con $\varepsilon$  que contenga 100 observaciones con una distribución $\varepsilon \sim\mathcal{N}(0,1)$. ¿Cuales son los coeficientes de regresión?. Estimar la correlación entre $x_1$ y $x_2$. Realizar un scatterplot en el que pueda observarse la relación entre $x_1$ y $x_2$. Utilizando los datos generados, ajustar a un modelo lineal para predecir $y$ en función de $x_1$ y $x_2$ , utilizando el método de cuadrados mı́nimos y comparar los valores exactos de $\beta$  con sus valores estimados.

```{r echo=TRUE}
X1 <- runif(n=100,min=0,max=1)
X2 <- runif(n=100,min=0,max=1)
e <- rnorm(n=100,mean=0,sd=1)
B0 <- 2
B1 <- 2
B2 <- 0.3
Y <- B0+B1*X1+B2*X2+e
```

_Para el modelo propuesto, los coeficientes de regresión son $\beta_0=$ `r B0`, $\beta_1=$ `r B1` y $\beta_2=$ `r B2`_ _La función `cor()` permite estimar la correlación entre dos vectores que para este caso resulta `cor(X1,X2)=` `r cor(X1,X2)`_

```{r echo=TRUE}
cor(X1,X2)
```

_Los vectores $X_1$ y $X_2$ son realizaciones de una muestra aleatoria con distribución uniforme y no deberían tener ninguna correlación como se puede ver en el siguiente gráfico de dispersión._

```{r }
DT <- data.table(x1=X1,x2=X2)
ggplot() + geom_point(data=DT, aes(x=x1, y=x2),color="blue")  + theme_bw()
```

```{r echo=TRUE}
DT <- data.table(x1=X1,x2=X2,y=Y)
MDL <- lm(data=DT,y~x1+x2)
b0 <- MDL$coefficients["(Intercept)"]
b1 <- MDL$coefficients["x1"]
b2 <- MDL$coefficients["x2"]
B0 <- 2
B1 <- 2
B2 <- 0.3
SUM <- data.table("\\beta_0 / \\hat{\\beta_0}"=B0/b0,"\\beta_1 / \\hat{\\beta_1}"=B1/b1,"\\beta_2 / \\hat{\\beta_2}"=B2/b2)
```
_La regresión mediante cuadrados mínimos se efectúa mediante el paquete 'lm()' y los estimadores de los oeficientes de regresión del modelo, resultan en $\hat{\beta_0}=$ `r b0`,$\hat{\beta_0}=$ `r b1` y $\hat{\beta_0}=$ `r b2`_

Comparar los valores exactos de $\beta_0$ Y $\beta_1$ con sus estimaciones. 

```{r}
  flextable(SUM) |>  colformat_double(digits = 4) |> 
  mk_par(part = "header", value = as_paragraph(as_equation(.)),use_dot = TRUE) |>    autofit()
```

  f) Repetir el inciso a) pero con el siguiente modelo:
  -   Crear dos vectores de datos de tamaño 100 $x_1$ y $x_2=0.5x_1+rnorm(100)/10$ a partir de una distribución uniforme en el intervalo (0, 1)  $X \sim\mathcal{U}(0,1)$
  -   Crear el vector $y = 2 + 2x_1 + 0,3x_2 + rnorm(100)$ 
  -   Comparar los resultados obtenidos con los del ítem a)



```{r echo=TRUE}
X1 <- runif(n=100,min=0,max=1)
X2 <- 0.5*X1+rnorm(n=100,mean=0,sd=1)/10 # == rnorm(n=100,mean=0,sd=0.1) ?
e <- rnorm(n=100,mean=0,sd=1)
B0 <- 2
B1 <- 2
B2 <- 0.3
Y <- B0+B1*X1+B2*X2+e
```

_En este ejemplo sólo el vector $X_1$ es una realización "pura" de una muestra aleatoria con distribución normal y el vector $X_2$ se genera como una combinación lineal de $X_1$ pero incluye un ruido aleatorio del 10% de la varianza de $X_1$, con lo cual la  correlación entre ambos resulta ahora más alta `cor(X1,X2)=` `r cor(X1,X2)`_


```{r }
DT <- data.table(x1=X1,x2=X2,y=Y)
ggplot(data=DT) +
  geom_point( aes(x=X1, y=X2),color="blue") +  theme_bw()
```
```{r echo=TRUE}
DT <- data.table(x1=X1,x2=X2,y=Y)
MDL <- lm(data=DT,y~x1+x2)
b0 <- MDL$coefficients["(Intercept)"]
b1 <- MDL$coefficients["x1"]
b2 <- MDL$coefficients["x2"]

```
_Los estimadores de los coeficientes de regresión del modelo, resultan ahora en $\hat{\beta_0}=$ `r b0`,$\hat{\beta_1}=$ `r b1` y $\hat{\beta_2}=$ `r b2`_


```{r}
SUM <- data.table("\\beta_0 / \\hat{\\beta_0}"=B0/b0,"\\beta_1 / \\hat{\\beta_1}"=B1/b1,"\\beta_2 / \\hat{\\beta_2}"=B2/b2)
  flextable(SUM) |>  colformat_double(digits = 4) |> 
  mk_par(part = "header", value = as_paragraph(as_equation(.)),use_dot = TRUE) |>    autofit()
```


